layers: [1, 256, 256, 256, 256, 256, 256, 256, 256, 5]
activation: ['tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh']
iterations: 500000
optimizer: adam
learning rate: 0.001
initial values: [1.390e-06 3.710e-06 1.080e-05 3.690e-05 1.775e-04]
training time: 32534.89019870758
residual: 1.8603141502170635e-05
best model at: 245000
train loss: 0.8279330617408568
