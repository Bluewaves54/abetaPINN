
layers: [1, 64, 64, 64, 64, 64, 5]
activation: ['tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh']
iterations: 75000
optimizer: adam
learning rate: 0.01
initial values: [1.390e-06 3.710e-06 1.080e-05 3.690e-05 1.775e-04]
training time: 1254.186064004898
residual: 37.58218270241749
best model at: 72000
train loss: 1.9835589013246366
