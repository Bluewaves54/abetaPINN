layers: [1, 64, 64, 64, 64, 64, 5]
activation: ['tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh']
iterations: 100000
optimizer: adam
learning rate: 0.0001
initial values: [1.390e-06 3.710e-06 1.080e-05 3.690e-05 1.775e-04]
training time: 1686.1469428539276
residual: 0.14510409849083983
best model at: 100000
train loss: 1.037056353267163
