
        layers: [1, 32, 32, 32, 5]

        activation: ['tanh', 'tanh', 'tanh', 'tanh']

        iterations: 70000

        optimizer: adam

        learning rate: 0.0001

        initial values: [1.390e-06 3.710e-06 1.080e-05 3.690e-05 1.775e-04]


    training time: 1037.58212184906
residual: 0.14897666777673285
best model at: 70000
train loss: 1.10742124538867
