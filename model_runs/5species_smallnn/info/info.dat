
layers: [1, 6, 6, 6, 5]
activation: ['tanh', 'tanh', 'tanh', 'tanh']
iterations: 50000
optimizer: adam
learning rate: 0.001
initial values: [1.390e-06 3.710e-06 1.080e-05 3.690e-05 1.775e-04]
training time: 944.4954416751862
residual: 0.2253432831308296
best model at: 45500
train loss: 1.9001948981222294
